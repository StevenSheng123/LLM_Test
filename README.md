"""
# LLM Eval Framework (vLLM-Based)

This repository supports evaluating large language models hosted with vLLM on benchmarks like MATH-500, AIME, MMLU, and more.

## Setup

```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

## Run Example (MATH-500)

```bash
python runners/run_math500.py
```
"""